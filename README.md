# Sparkify Data Warehouse Project

This project is a part of Udacity course Data Engineering with AWS. It involves processing data for a music streaming startup, Sparkify. The data, including a song and artist library with metadata, as well as event logs, is stored on Amazon S3 in JSON format. The goal of the project is to extract data from these sources and transform it into a star schema in a Redshift data warehouse database for analytical processes. The project consists of three main steps:

1. **Load Data from S3 to Staging Tables in Redshift:** There are two staging tables that hold two types of data. Data on songs and artists are located in the `staging_songs` table, and data on user activity is located in the `staging_events` table. The original data on songs and artists in the `staging_songs` table contains rows with the same `artist_id` but corresponding to different `artist_name`. To ensure data uniformity, all artists with the same ID were given the same name, chosen as the most frequent name for that ID.

2. **Create Star Schema in Sparkify Database:** The star schema is created, consisting of a fact table, `songplay`, based on user listening activity, and four dimensional tables with metadata on users, songs, artists, and time. This format provides efficient querying and reporting and is commonly used in data warehousing and business intelligence.

3. **Populate the Star Schema:** The star schema is then populated using INSERT statements with entries from staging tables. To perform JOIN operations on the data from the two original sources and map user activity to song metadata, the tables are joined based on the combination of artist and title of the song.

The logic of the ETL pipeline is implemented in three modules:
- `sql_queries.py`: Contains all the SQL statements used to operate on the data by being imported into the other two files.
- `create_tables.py`: Responsible for dropping and creating all the tables.
- `etl.py`: Implements the final pipeline by calling functions that perform loading and inserting of the data. Additionally, there's an `investigate()` function that performs some exemplary analytical SQL queries on the data in the star schema database.

## Datasets

The datasets used in the project are taken from the [Million Song Dataset](http://millionsongdataset.com/index.html) and the log dataset was generated by the [event simulator](https://github.com/Interana/eventsim). Both datasets were provided by the Udacity course Data Engineering with AWS.